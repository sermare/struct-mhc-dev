{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa339d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_pwm(seqs, length=9):\n",
    "    aa = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    pwm = np.zeros((20, length))\n",
    "    for s in seqs:\n",
    "        if len(s) != length:\n",
    "            continue\n",
    "        for i, res in enumerate(s):\n",
    "            if res not in aa:         # skip non-canonical\n",
    "                break\n",
    "            pwm[aa.index(res), i] += 1\n",
    "    pwm = pwm / pwm.sum(axis=0, keepdims=True)          # column normalise\n",
    "    return pwm\n",
    "\n",
    "def max_identity(query, ref_set):\n",
    "    if len(query) in {len(r) for r in ref_set}:         # fast path – same length\n",
    "        return max(sum(q==r for q, r in zip(query, ref)) / len(query)\n",
    "                   for ref in ref_set)\n",
    "    # otherwise do Smith–Waterman (score=1, mismatch=0, no gaps)\n",
    "    best = 0\n",
    "    for ref in ref_set:\n",
    "        aln = pairwise2.align.localms(query, ref, 1, 0, -10, -10, one_alignment_only=True)\n",
    "        if aln:\n",
    "            _, _, score, _, _ = aln[0]\n",
    "            best = max(best, score / max(len(query), len(ref)))\n",
    "    return best\n",
    "\n",
    "# 1. split sets\n",
    "testing_set  = training_data\n",
    "generated_set = final_df[final_df['measured'].isna()][['sequence', 'HLA']]\n",
    "\n",
    "testing_pep   = testing_set['sequence'].tolist()\n",
    "generated_pep = generated_set['sequence'].tolist()\n",
    "\n",
    "# 2. Jensen–Shannon divergence on PWMs (length 9 by default)\n",
    "L = 9\n",
    "pwm_test = build_pwm(testing_pep, length=L)\n",
    "pwm_gen  = build_pwm(generated_pep, length=L)\n",
    "\n",
    "# flatten 20×L → 20L vector, add small ε to avoid log(0)\n",
    "eps  = 1e-9\n",
    "jsd  = jensenshannon(pwm_test.flatten()+eps, pwm_gen.flatten()+eps, base=2)\n",
    "print(f\"JS-divergence between measured and RFdiffusion PWMs (length {L}): {jsd:.4f} bits\")\n",
    "\n",
    "# 3. max identity distribution (might take a few minutes for large sets)\n",
    "ref_set = set(testing_pep)                       # for O(1) exact look-ups\n",
    "identity_scores = [max_identity(p, ref_set) for p in generated_pep]\n",
    "\n",
    "# save for plotting (e.g., seaborn.histplot)\n",
    "id_series = pd.Series(identity_scores, name='max_identity_to_training')\n",
    "# id_series.to_csv('/global/scratch/users/sergiomar10/ESMCBA/analysis/max_identity_generated_vs_training.csv',\n",
    "#                  index=False)\n",
    "id_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da7da7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random, string, pandas as pd\n",
    "aa = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "def mutate_keep_anchors(native_pep, n_mutants=50, anchors=(1,8)):\n",
    "    \"\"\"anchors are 0-indexed positions to keep (P2=1, PΩ=8 for 9-mers).\"\"\"\n",
    "    mutants = []\n",
    "    for _ in range(n_mutants):\n",
    "        s = list(native_pep)\n",
    "        for i in range(len(s)):\n",
    "            if i in anchors:           # keep anchor residue\n",
    "                continue\n",
    "            s[i] = random.choice(aa)\n",
    "        mutants.append(\"\".join(s))\n",
    "    return mutants\n",
    "\n",
    "# example\n",
    "native_peptide = \"LLFGYPVYV\"           # A*02:01 binder in 1AKJ\n",
    "mutants = mutate_keep_anchors(native_peptide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd390a42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch, numpy as np\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\").eval()\n",
    "\n",
    "def sample_9mers(n_samples=5000, temperature=1.0):\n",
    "    # Encode BOS + EOS special tokens\n",
    "    bos = tok.cls_token_id\n",
    "    eos = tok.eos_token_id\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            seq_ids = [bos]\n",
    "            while True:\n",
    "                logits = model(torch.tensor(seq_ids)[None, :]).logits[0, -1, :] / temperature\n",
    "                next_id = torch.multinomial(torch.softmax(logits, dim=-1), 1).item()\n",
    "                if next_id == eos or len(seq_ids) > 9:\n",
    "                    break\n",
    "                seq_ids.append(next_id)\n",
    "            if len(seq_ids) == 10:                    # 9 aa + BOS\n",
    "                samples.append(tok.decode(seq_ids[1:]))\n",
    "    return samples\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
